{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from train import *\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment settings\n",
    "Settings are based on the training procedure that produced the final generator used in evaluating the stylized facts and training the RL agent in portfolio management.  \n",
    "However, using different machines would result in cumulative differences in computation results likely due to varied machine precision.  \n",
    "Original experiments were performed on machine with following specs:\n",
    "1. OS: Ubuntu 20.04.1\n",
    "2. CPU: Intel(R) Xeon(R) Gold 6248 CPU @ 2.50GHz\n",
    "3. GPU: Tesla V100-SXM2-32GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "batch_size = 38 # number of samples in each batch\n",
    "sample_len = 300 # length of each sample\n",
    "sample_model = 'Realdt' # GBM, Heston, OU, RealData, Realdt, spx_rates\n",
    "lead_lag = True # whether to use lead lag transformation\n",
    "lags = [1] # number of lags to use for lead lag transformation: int or list[int]\n",
    "seed = 42\n",
    "\n",
    "# real data parameters\n",
    "stride = 50 # for real data\n",
    "start_date = '1995-01-01' # start date for real data\n",
    "end_date = '2018-09-18' # end date for real data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and kernel related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signature kernel\n",
    "static_kernel_type = 'rq' # type of static kernel to use - rbf, rbfmix, rq, rqmix, rqlinear for\n",
    "n_levels = 10 # number of levels in the truncated signature kernel\n",
    "\n",
    "# generator\n",
    "seq_dim = 1 # dimension of sequence vector\n",
    "activation = 'Tanh' # pytorch names e.g. Tanh, ReLU. NOTE: does NOT change transformer layers'\n",
    "hidden_size = 64\n",
    "n_lstm_layers = 1 # number of LSTM layers\n",
    "conditional = True # feed in history for LSTM generators\n",
    "hist_len = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 4 # dimension of noise vector\n",
    "ma = True # whether to use MA noise generator fitted to log returns gaussianized by Lambert W transformation\n",
    "ma_p = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000 # number of batches\n",
    "start_lr = 0.001 # starting learning rate\n",
    "patience = 100 # number of epochs to wait before reducing lr\n",
    "lr_factor = 0.5 # factor to multiply lr by for scheduler\n",
    "early_stopping = patience*3 # number of epochs to wait before no improvement\n",
    "kernel_sigma = 0.1 # starting kernel_sigma\n",
    "num_losses = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to tensorboard log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all parameters to a dictionary\n",
    "rng = np.random.default_rng(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data_params, model_params, train_params = get_params_dicts(vars().copy())\n",
    "\n",
    "# save parameters to tensorboard\n",
    "writer = start_writer(data_params, model_params, train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, kernel, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 5855.988337142852\n",
      "            Iterations: 37\n",
      "            Function evaluations: 859\n",
      "            Gradient evaluations: 37\n",
      "                        Zero Mean - ARCH Model Results                        \n",
      "==============================================================================\n",
      "Dep. Variable:           gaussianized   R-squared:                       0.000\n",
      "Mean Model:                 Zero Mean   Adj. R-squared:                  0.000\n",
      "Vol Model:                       ARCH   Log-Likelihood:               -5855.99\n",
      "Distribution:                  Normal   AIC:                           11754.0\n",
      "Method:            Maximum Likelihood   BIC:                           11897.9\n",
      "                                        No. Observations:                 6999\n",
      "Date:                Sun, Sep 01 2024   Df Residuals:                     6999\n",
      "Time:                        13:49:04   Df Model:                            0\n",
      "                               Volatility Model                              \n",
      "=============================================================================\n",
      "                 coef    std err          t      P>|t|       95.0% Conf. Int.\n",
      "-----------------------------------------------------------------------------\n",
      "omega          0.0727  8.049e-03      9.032  1.688e-19  [5.692e-02,8.848e-02]\n",
      "alpha[1]       0.0186  1.285e-02      1.449      0.147 [-6.566e-03,4.380e-02]\n",
      "alpha[2]       0.0827  1.292e-02      6.401  1.546e-10    [5.736e-02,  0.108]\n",
      "alpha[3]       0.0258  1.273e-02      2.028  4.254e-02  [8.689e-04,5.075e-02]\n",
      "alpha[4]       0.0722  1.427e-02      5.061  4.163e-07    [4.426e-02,  0.100]\n",
      "alpha[5]       0.1752  1.634e-02     10.721  8.127e-27      [  0.143,  0.207]\n",
      "alpha[6]       0.0267  1.268e-02      2.101  3.562e-02  [1.792e-03,5.151e-02]\n",
      "alpha[7]       0.0282  1.428e-02      1.978  4.795e-02  [2.544e-04,5.621e-02]\n",
      "alpha[8]       0.0191  1.346e-02      1.419      0.156 [-7.277e-03,4.547e-02]\n",
      "alpha[9]       0.0556  1.477e-02      3.764  1.673e-04  [2.664e-02,8.452e-02]\n",
      "alpha[10]      0.1145  1.513e-02      7.571  3.714e-14    [8.487e-02,  0.144]\n",
      "alpha[11]      0.0196  1.386e-02      1.414      0.157 [-7.571e-03,4.676e-02]\n",
      "alpha[12]  4.4795e-03  1.334e-02      0.336      0.737 [-2.166e-02,3.062e-02]\n",
      "alpha[13]  8.0320e-12  1.147e-02  7.002e-10      1.000 [-2.248e-02,2.248e-02]\n",
      "alpha[14]      0.0266  1.424e-02      1.868  6.179e-02 [-1.312e-03,5.449e-02]\n",
      "alpha[15]      0.0318  1.366e-02      2.329  1.986e-02  [5.041e-03,5.858e-02]\n",
      "alpha[16]  7.9793e-12  1.425e-02  5.599e-10      1.000 [-2.793e-02,2.793e-02]\n",
      "alpha[17]  9.4517e-12  1.227e-02  7.703e-10      1.000 [-2.405e-02,2.405e-02]\n",
      "alpha[18]  1.2805e-11  1.414e-02  9.055e-10      1.000 [-2.771e-02,2.771e-02]\n",
      "alpha[19]      0.0806  1.513e-02      5.331  9.787e-08    [5.099e-02,  0.110]\n",
      "alpha[20]      0.0205  1.290e-02      1.588      0.112 [-4.793e-03,4.577e-02]\n",
      "=============================================================================\n",
      "\n",
      "Covariance estimator: robust\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenLSTM(\n",
       "  (rnn): LSTM(6, 64, batch_first=True)\n",
       "  (mean_net): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (var_net): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       "  (output_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = get_dataloader(**{**data_params, **model_params})\n",
    "kernel = get_signature_kernel(**{**model_params, **train_params})\n",
    "generator = get_generator(**{**model_params, **data_params})\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MMD-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hous/Github/Generative-Model-Signature-MMD/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "100%|██████████| 3/3 [00:57<00:00, 19.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 711912.9375, avg_last_20_loss: 711912.9375\n",
      "Saving model at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdata_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/train.py:215\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, kernel, dataloader, rng, writer, device, checkpoint, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# compute loss\u001b[39;00m\n\u001b[1;32m    214\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 215\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_mmd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlead_lag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# backpropagate and update weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/train.py:134\u001b[0m, in \u001b[0;36mcompute_mmd_loss\u001b[0;34m(kernel, X, output, lead_lag, lags)\u001b[0m\n\u001b[1;32m    132\u001b[0m     X \u001b[38;5;241m=\u001b[39m batch_lead_lag_transform(X[:,:,\u001b[38;5;241m1\u001b[39m:], X[:,:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], lags) \u001b[38;5;66;03m# inputs are (price series, time dimension, lags to use)\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     output \u001b[38;5;241m=\u001b[39m batch_lead_lag_transform(output[:,:,\u001b[38;5;241m1\u001b[39m:], output[:,:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], lags)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mksig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmmd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/sigkernel/loss.py:14\u001b[0m, in \u001b[0;36mmmd_loss\u001b[0;34m(X, Y, kernel)\u001b[0m\n\u001b[1;32m     12\u001b[0m K_XX \u001b[38;5;241m=\u001b[39m kernel(X,X)\n\u001b[1;32m     13\u001b[0m K_YY \u001b[38;5;241m=\u001b[39m kernel(Y,Y)\n\u001b[0;32m---> 14\u001b[0m K_XY \u001b[38;5;241m=\u001b[39m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# unbiased MMD statistic (could also use biased, doesn't matter if we use permutation tests)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(K_XX)\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/sigkernel/kernels.py:96\u001b[0m, in \u001b[0;36mSignatureKernel.__call__\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     94\u001b[0m R \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclone(M)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_levels):\n\u001b[0;32m---> 96\u001b[0m     R \u001b[38;5;241m=\u001b[39m M \u001b[38;5;241m*\u001b[39m \u001b[43mmulti_cumsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     K \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(R, dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/sigkernel/kernels.py:122\u001b[0m, in \u001b[0;36mmulti_cumsum\u001b[0;34m(M, axis)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# pre-pad with zeros along the given axis if exclusive cumsum\u001b[39;00m\n\u001b[1;32m    121\u001b[0m pads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(x \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(ndim)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m--> 122\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m M\n",
      "File \u001b[0;32m~/Github/Generative-Model-Signature-MMD/.venv/lib/python3.10/site-packages/torch/nn/functional.py:4495\u001b[0m, in \u001b[0;36mpad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4488\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pad) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplicate\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   4489\u001b[0m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[1;32m   4490\u001b[0m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[1;32m   4491\u001b[0m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[1;32m   4492\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch._decomp.decompositions\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplication_pad2d(\n\u001b[1;32m   4493\u001b[0m                 \u001b[38;5;28minput\u001b[39m, pad\n\u001b[1;32m   4494\u001b[0m             )\n\u001b[0;32m-> 4495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(generator, kernel, dataloader, rng, writer, device, **{**train_params, **model_params, **data_params})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
